import csv csvfile = sc.textFile('NYPD.csv') 
data = csvfile.mapPartitions(lambda x: csv.reader(x))

schema_sd = spark.createDataFrame(data, ('CMPLNT_NUM' ,'CMPLNT_FR_DT' ,'CMPLNT_FR_TM' ,'CMPLNT_TO_DT' ,'CMPLNT_TO_TM' ,'RPT_DT' ,'KY_CD' ,'OFNS_DESC' ,'PD_CD' ,'PD_DESC' ,'CRM_ATPT_CPTD_CD' ,'LAW_CAT_CD' ,'JURIS_DESC' ,'BORO_NM' ,'ADDR_PCT_CD' ,'LOC_OF_OCCUR_DESC' ,'PREM_TYP_DESC' ,'PARKS_NM' ,'HADEVELOPT' ,'X_COORD_CD' ,'Y_COORD_CD' ,'Latitude' ,'Longitude' ,'Lat_Lon' ))

schema_sd.createOrReplaceTempView("nypd_complaints")

import csv csvfile = sc.textFile('Police_officials_monthly.csv') 
data1 = csvfile.mapPartitions(lambda x: csv.reader(x))

schema_sd1 = spark.createDataFrame(data1, ('month' ,'year' ,'count'))

schema_sd1.createOrReplaceTempView("police_officials_monthly")

df =spark.sql("select CONCAT(b.year,MONTHNAME(STR_TO_DATE(b.month,'%m'))) as period, count ,incidences  from Police_Officials_monthly b 
JOIN 
(
SELECT CONCAT(YEAR(STR_TO_DATE(RPT_DT, '%m/%d/%Y')),MONTH(STR_TO_DATE(RPT_DT, '%m/%d/%Y'))) period,count(1) incidences from nypd_complaints
group by CONCAT(YEAR(STR_TO_DATE(RPT_DT, '%m/%d/%Y')),MONTH(STR_TO_DATE(RPT_DT, '%m/%d/%Y'))) 
order by YEAR(STR_TO_DATE(RPT_DT, '%m/%d/%Y')),MONTH(STR_TO_DATE(RPT_DT, '%m/%d/%Y')) 
) c on CONCAT(b.year,b.month)=c.period
order by b.year,b.month")
 
df.write.csv('Police_officials_Hypothesis.csv')